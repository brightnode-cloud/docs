---
title: "Generate images with ComfyUI"
sidebarTitle: "Generate images with ComfyUI"
description: "Deploy ComfyUI on Brightnode to create AI-generated images."
tag: "NEW"
---

This tutorial walks you through how to configure ComfyUI on a [GPU Node](/nodes/overview) and use it to generate images with text-to-image models.

[ComfyUI](https://www.comfy.org/) is a node-based graphical interface for creating AI image generation workflows. Instead of writing code, you connect different components visually to build custom image generation pipelines. This approach provides flexibility to experiment with various models and techniques while maintaining an intuitive interface.

This tutorial uses the [SDXL-Turbo](https://huggingface.co/stabilityai/sdxl-turbo) model and a matching template, but you can adapt these instructions for any model/template combination you want to use.

<Tip>
When you're just getting started with ComfyUI, it's important to use a workflow that was created for the specific model you intend to use. You usually can't just switch the "Load Checkpoint" node from one model to another and expect optimal performance or results.

For example, if you load a workflow created for the Flux Dev model and try to use it with SDXL-Turbo, the workflow might run, but with poor speed or image quality.
</Tip>

## What you'll learn

In this tutorial, you'll learn how to:

- Deploy a Node with ComfyUI pre-installed.
- Connect to the ComfyUI web interface.
- Browse pre-configured workflow templates.
- Install new models to your Node.
- Generate an image.

## Requirements

Before you begin, you'll need:

- A [Brightnode account](/get-started/manage-accounts).
- At least $10 in Brightnode credits.
- A basic understanding of AI image generation.

## Step 1: Deploy a ComfyUI Node

First, you'll deploy a Node using a template that pre-installs ComfyUI and the ComfyUI Manager plugin:

<Steps>
  <Step title="Deploy a Node from the ComfyUI template">
    Go to the [Better Comfy Slim](https://console.brightnode.cloud/explore/cndsag8ob0) template in the Brightnode console and click **Deploy**.
    
    <Note>
    There are several ComfyUI templates available in the Brightnode console, many with pre-installed models. We're using this one because it's extremely lightweight, but it comes with the ComfyUI Manager plugin pre-installed so you can easily install the specific models/nodes you need after deployment.
    </Note>
  </Step>

  <Step title="Configure your Node">
    Configure your Node with these settings:
    
    - **GPU selection:** Choose an L40 or RTX 4090 for optimal performance with SDXL-Turbo. Lower VRAM GPUs may work for smaller models.
    - **Storage:** The default container and volume disk sizes set by the template should be sufficient for SDXL-Turbo. You can also add a [network volume](/storage/network-volumes) to your Node if you want persistent storage.
    - **Deployment type:** Select **On-Demand** for flexibility.
  </Step>

  <Step title="Deploy and wait">
    Click **Deploy On-Demand** to create your Node.
    
    The Node can take up to 30 minutes to initialize the container and start the ComfyUI HTTP service.
  </Step>
</Steps>

## Step 2: Open the ComfyUI interface

Once your Node has finished initializing, you can open the ComfyUI interface:

<Steps>
  <Step title="Open your Node in the console">
    Go to the [Nodes section](https://www.brightnode.cloud/console/nodes) in the Brightnode console, then find your deployed ComfyUI Node and expand it.

    <Warning>
    The Node may take up to 30 minutes to initialize when first deployed. Future starts will generally take much less time.
    </Warning>
  </Step>

  <Step title="Start the ComfyUI service">
    Click **Connect** on your Node, then select the last HTTP service button in the list, labeled **Connect to HTTP Service [Port 8188]**.

    This will open the ComfyUI interface in a new browser tab. The URL follows the format: `https://[POD_ID]-8188.proxy.brightnode.net`.

    <Note>
    If you see the label "Not Ready" on the HTTP service button, or you get a "Bad Gateway" error when first connecting, wait 2â€“3 minutes for the service to fully start, then refresh the page.
    </Note>

  </Step>

</Steps>

## Step 3: Load a workflow template

ComfyUI workflows consist of a series of nodes that are connected to each other to create a AI generation pipeline. Rather than creating our own workflow from scratch, we'll load a pre-configured workflow that was created for the specific model we intend to use:

<Steps>
  <Step title="Open the template browser">
   When you first open the ComfyUI interface, the template browser should open automatically. If it doesn't, click the **Workflow** button in the top right corner of the ComfyUI interface, then select **Browse Templates**.
  </Step>

  <Step title="Load the SDXL-Turbo template">
    In the sidebar to the left of the browser, select the **Image** tab. Find the **SDXL-Turbo** template and click on it to load a basic image generation workflow.
  </Step>
</Steps>

## Step 4: Install the SDXL-Turbo model

As soon as you load the workflow, you'll see a popup labeled **Missing Models**. This happens because the Node template we deployed doesn't come pre-installed with any models, so we'll need to install them now.

Rather than clicking the download button (which downloads the missing model to your local machine), use the ComfyUI Manager plugin to install the missing model directly onto the Node:

<Steps>
  <Step title="Open the ComfyUI Manager">
    Close the **Missing Models** popup by clicking the **X** in the top right corner. Then click **Manager** in the top right of the ComfyUI interface, and select **Model Manager** from the list of options.
  </Step>

  <Step title="Install the SDXL-Turbo model checkpoint">
    In the search bar, enter `SDXL-Turbo 1.0 (fp16)`, then click **Install**.
  </Step>

  <Step title="Refresh the interface">
    Before you can use the model, you'll need to refresh the ComfyUI interface. You can do this by either refreshing the browser tab where it's running, or by pressing <kbd>R</kbd>.
  </Step>

  <Step title="Configure the checkpoint node">
    Find the node labeled **Load Checkpoint** in the workflow. It should be the first node on the left side of the canvas.

    Click on the dropdown menu labeled `ckpt_name` and select the SDXL-Turbo model checkpoint you just installed (named `SDXL-TURBO/sd_xl_turbo_1.0_fp16.safetensors`).
  </Step>
</Steps>

## Step 5: Generate an image

Your workflow is now ready! Follow these steps to generate an image:

<Steps>
  <Step title="Customize your prompt">
    Locate the text input node labeled **CLIP Text Encode (Prompt)** in the workflow. 
    
    Click on the text field containing the default prompt and replace it with your desired image description.
    
    Example prompts:
    - "A serene mountain landscape at sunset with a crystal clear lake."
    - "A futuristic cityscape with neon lights and flying vehicles."
    - "A detailed portrait of a robot reading a book in a library."
  </Step>

  <Step title="Start generation">
    Click **Run** at the bottom of the workflow (or press <kbd>Ctrl</kbd>+<kbd>Enter</kbd>) to begin the image generation process.
    
    Watch as the workflow progresses through each node:
    - Text encoding.
    - Model loading.
    - Image generation steps.
    - Final output processing.

    <Note>
    The first generation may take a few minutes to complete as the model checkpoint must be loaded. Subsequent generations will be much faster.
    </Note>

  </Step>

  <Step title="View your result">
    The generated image appears in the output node when complete. 
    
    Right-click the image to save it to your local machine, view it at full resolution, or copy it to your clipboard.
  </Step>
</Steps>

Congratulations! You've just generated your first image with ComfyUI on Brightnode.

## Troubleshooting

Here are some common issues you may encounter and possbile solutions:

- **Connection errors**: Wait for the Node to fully initialize (up to 30 minutes for initial deployment).
- **HTTP service not ready**: Wait at least 2 to 3 minutes after Node deployment for the HTTP service to fully start. You can also check the Node logs in the Brightnode console to look for deployment errors.
- **Out of memory errors**: Reduce image resolution or batch size in your workflow.
- **Slow generation**: Make sure you're using an appropriate GPU for your selected model. See [Choose a Node](/nodes/choose-a-node) for guidance.

## Next steps

Once you're comfortable with basic image generation, explore the [ComfyUI documentation](https://docs.comfy.org/) to learn how to build more advanced workflows.

Here are some ideas for where to start:

### Experiment with different workflow templates

Use the template browser from [Step 3](#step-3%3A-load-a-workflow-template) to test out new models and find a workflow that suits your needs.

You can also browse the web for a preconfigured workflow and import it by clicking **Workflow** in the top right corner of the ComfyUI interface, selecting **Open**, then selecting the workflow file you want to import.

Don't forget to install any missing models using the model manager. If you need a model that isn't available in the model manager, you can download it from the web to your local machine, then use the [Brightnode CLI](/brightnodectl/overview) to transfer the model files directly into your Node's `/workspace/madapps/ComfyUI/models` directory.

### Create custom workflows

Build your own workflows by:

1. Right-clicking the canvas to add new nodes.
2. Connecting node outputs to inputs by dragging between connection points.
3. Saving your custom workflow with <kbd>Ctrl</kbd>+<kbd>S</kbd> or by clicking **Workflow** and selecting **Save**.

### Manage your Node

While working with ComfyUI, you can monitor your usage by checking GPU/disk utilization in the [Nodes page](https://console.brightnode.cloud/nodes) of the Brightnode console.

Stop your Node when you're finished to avoid unnecessary charges.

It's also a good practice to download any custom workflows to your local machine before stopping the Node. For persistent storage of models and outputs across sessions, consider using a [network volume](/storage/network-volumes).